{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdeafae-3aa5-4912-9dba-55d5c86ec18e",
   "metadata": {},
   "source": [
    "# Raw data preparation\n",
    "\n",
    "- creating dataframes\n",
    "- cleaning dataframes\n",
    "- normalizing dataframes\n",
    "- assigning labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0873a-0faf-408c-9b87-e9a6c2ce38e8",
   "metadata": {},
   "source": [
    "The raw data is in NetCDF4 format. The code below pre-processes the data per climatological year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a9857-b71e-441b-a797-2254f4bebbae",
   "metadata": {},
   "source": [
    "## Creating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7d20c6-db73-40fc-8650-1043ecfd2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_heatwave_df(year):\n",
    "    file_pattern = '../../private/is_heatwave/Europe_0.25deg/Europe_0.25deg_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    \n",
    "    ds = xr.open_mfdataset(file_paths)\n",
    "\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Save the merged data to a new NetCDF file\n",
    "    if os.path.exists(\"data/\" + year + \"/ih_\" + year + \".csv\"):\n",
    "        os.remove(\"data/\" + year + \"/ih_\" + year + \".csv\")\n",
    "        df.to_csv(\"data/\" + year + \"/ih_\" + year + \".csv\", index=False)\n",
    "    else:\n",
    "        df.to_csv(\"data/\" + year + \"/ih_\" + year + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7330df46-c198-47ee-b21d-83d493a231e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_net_df(year):\n",
    "    run1 = 20240122 if int(year) <= 1999 else (20240105 if 2000 <= int(year) <= 2009 else 20231214)\n",
    "    run2 = 1017 if int(year) <= 1999 else (1808 if 2000 <= int(year) <= 2009 else 2202)\n",
    "    \n",
    "    run1 = str(run1)\n",
    "    run2 = str(run2)\n",
    "    \n",
    "    # BC data\n",
    "    file_pattern = '../../data/latest/iw-heatwaves-storage-master/output/master/' + run1 + '_' + run2 + '/rasterfiles/Europe/' + year + '/CN_Europe_0.25x0.25deg_BC_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    bc_2003 = xr.open_mfdataset(file_paths) # Open and concatenate files\n",
    "    bc_2003 = bc_2003.rename({'coefficient': 'BC'}) # Rename coefficient\n",
    "\n",
    "    # CC data\n",
    "    file_pattern = '../../data/latest/iw-heatwaves-storage-master/output/master/' + run1 + '_' + run2 + '/rasterfiles/Europe/' + year + '/CN_Europe_0.25x0.25deg_CC_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    cc_2003 = xr.open_mfdataset(file_paths) # Open and concatenate files\n",
    "    cc_2003 = cc_2003.rename({'coefficient': 'CC'}) # Rename coefficient\n",
    "\n",
    "    # DC data\n",
    "    file_pattern = '../../data/latest/iw-heatwaves-storage-master/output/master/' + run1 + '_' + run2 + '/rasterfiles/Europe/' + year + '/CN_Europe_0.25x0.25deg_DC_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    dc_2003 = xr.open_mfdataset(file_paths) # Open and concatenate files\n",
    "    dc_2003 = dc_2003.rename({'coefficient': 'DC'}) # Rename coefficient\n",
    "\n",
    "    # ID data\n",
    "    file_pattern = '../../data/latest/iw-heatwaves-storage-master/output/master/' + run1 + '_' + run2 + '/rasterfiles/Europe/' + year + '/CN_Europe_0.25x0.25deg_ID_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    id_2003 = xr.open_mfdataset(file_paths) # Open and concatenate files\n",
    "    id_2003 = id_2003.rename({'coefficient': 'ID'}) # Rename coefficient\n",
    "\n",
    "    # OD data\n",
    "    file_pattern = '../../data/latest/iw-heatwaves-storage-master/output/master/' + run1 + '_' + run2 + '/rasterfiles/Europe/' + year + '/CN_Europe_0.25x0.25deg_OD_' + year + '-*.nc'\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    od_2003 = xr.open_mfdataset(file_paths) # Open and concatenate files\n",
    "    od_2003 = od_2003.rename({'coefficient': 'OD'}) # Rename coefficient\n",
    "\n",
    "    # Merge datasets\n",
    "    ds = xr.combine_by_coords([bc_2003, cc_2003, dc_2003, id_2003, od_2003])\n",
    "\n",
    "    # Convert to pandas\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "    df = df.drop('spatial_ref', axis=1)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Save the merged data to a new csv file\n",
    "    if os.path.exists(\"data/\" + year + \"/merged_\" + year + \".csv\"):\n",
    "        os.remove(\"data/\" + year + \"/merged_\" + year + \".csv\")\n",
    "        df.to_csv(\"data/\" + year + \"/merged_\" + year + \".csv\", index=False)\n",
    "    else:\n",
    "        df.to_csv(\"data/\" + year + \"/merged_\" + year + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fd6c7-928b-48c0-9747-4872f4d574a0",
   "metadata": {},
   "source": [
    "## Cleaning dataframes (and merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a451f3b-b19f-4d05-9b82-72ddbf3abb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_lat_lon(df):\n",
    "  grouped = df.groupby(['lat', 'lon', 'time'])\n",
    "  duplicate_rows = grouped.filter(lambda x: len(x) > 1)\n",
    "  unique_rows = df[~df.index.isin(duplicate_rows.index)]\n",
    "\n",
    "  return duplicate_rows, unique_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a306878-18bd-4a86-8cd1-c24f9feecb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_uneven(df):\n",
    "    dup_rows, uni_rows = dup_lat_lon(df)\n",
    "    dup_rows = dup_rows.reset_index(drop=True)\n",
    "\n",
    "    even_df = dup_rows[dup_rows.index % 2 == 0]\n",
    "    uneven_df = dup_rows[dup_rows.index % 2 != 0]\n",
    "    \n",
    "    even = pd.concat([uni_rows, even_df], ignore_index=True)\n",
    "    uneven = pd.concat([uni_rows, uneven_df], ignore_index=True)\n",
    "    \n",
    "    return even, uneven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5055a96d-e350-4b84-aa7b-a4588618f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df1, df2):\n",
    "    # Check for difference in time column\n",
    "    time_difference = list(set(sorted(df2.time.unique())).difference(sorted(df1.time.unique())))\n",
    "    df = df2[~df2['time'].isin(time_difference)]\n",
    "\n",
    "    # Check for duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Identify stuborn duplicates\n",
    "    stub_dup = df.groupby(['lat', 'lon', 'time']).size().reset_index(name='count_')\n",
    "    dup_dates = stub_dup[stub_dup['count_'] == 2].time.unique()\n",
    "    df_sub = df[~df['time'].isin(dup_dates)]\n",
    "    df_even = pd.DataFrame()\n",
    "    df_uneven = pd.DataFrame()\n",
    "    for i in range(len(dup_dates)):\n",
    "        df_stub = df[df['time'] == dup_dates[i]]\n",
    "        even, uneven = even_uneven(df_stub)\n",
    "        df_even = pd.concat([df_even, even], ignore_index=True)\n",
    "        df_uneven = pd.concat([df_uneven, uneven], ignore_index=True)\n",
    "    df_zero = pd.concat([df_sub, df_even], ignore_index=True)\n",
    "    df_notzero = pd.concat([df_sub, df_uneven], ignore_index=True)\n",
    "    \n",
    "    return df_zero, df_notzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2d7b4e-8e8c-4ce2-bd3d-01a4ae286d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(year):\n",
    "    df1 = pd.read_csv('data/' + year + '/ih_' + year + '.csv')\n",
    "    df2 = pd.read_csv('data/' + year + '/merged_' + year + '.csv')\n",
    "\n",
    "    df_zero, df_notzero = clean(df1, df2)\n",
    "\n",
    "    df1 = df1.sort_values(by=['y', 'x', 'time'], ascending=[True, True, True])\n",
    "    df_zero = df_zero.sort_values(by=['lon', 'lat', 'time'], ascending=[True, True, True])\n",
    "    df_notzero = df_notzero.sort_values(by=['lon', 'lat', 'time'], ascending=[True, True, True])\n",
    "\n",
    "    md_zero = pd.concat([df1['is_heatwave'], df_zero], axis=1)\n",
    "    md_notzero = pd.concat([df1['is_heatwave'], df_notzero], axis=1)\n",
    "\n",
    "    md_zero.to_csv(\"data/\" + year + \"/md_zero_\" + year + \".csv\", index=False)\n",
    "    md_notzero.to_csv(\"data/\" + year + \"/md_!zero_\" + year + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814915d-27a8-4d1d-9426-a369dd981247",
   "metadata": {},
   "source": [
    "## Normalizing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cc8188-dbb1-4c04-8cd8-ee0de927c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(year):\n",
    "    df1 = pd.read_csv('data/' + year + '/md_zero_' + year + '.csv')\n",
    "    df2 = pd.read_csv('data/' + year + '/md_!zero_' + year + '.csv')\n",
    "\n",
    "    # Select columns to normalize\n",
    "    columns = df1.columns.difference(['is_heatwave', 'lon', 'lat', 'time'])\n",
    "    \n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df1[columns] = scaler.fit_transform(df1[columns])\n",
    "    df2[columns] = scaler.fit_transform(df2[columns])\n",
    "\n",
    "    df1.to_csv('data/' + year + '/md_zero_' + year + '_norm.csv', index=False)\n",
    "    df2.to_csv('data/' + year + '/md_!zero_' + year + '_norm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6938b1-ff6b-4f5b-b105-616874577dc0",
   "metadata": {},
   "source": [
    "## Labeling dataframes (sources and sinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cdcb19-3c5d-440d-a863-cd8192d50974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_sinks(df):\n",
    "    time = sorted(df.time.unique())\n",
    "    data = []\n",
    "\n",
    "    for i in time:\n",
    "        day = df[df['time'] == i].reset_index(drop=True)\n",
    "        day['Type'] = ''\n",
    "        index_set = set(range(len(set(day.index))))\n",
    "\n",
    "        # Identify coefficients source and sink values\n",
    "        bc_source = np.where(day['BC'] == day['BC'].max())\n",
    "        id_source = np.where(day['ID'] == day['ID'].min()) \n",
    "        od_source = np.where(day['OD'] == day['OD'].max()) # high OD is a source!!\n",
    "        \n",
    "        bc_sink = np.where(day['BC'] == df.BC.drop_duplicates().nsmallest(2).iloc[-1]) # the second smallest value\n",
    "        id_sink = np.where(day['ID'] == day['ID'].max()) # high ID is a sink!!\n",
    "        od_sink = np.where(day['OD'] == day['OD'].min()) \n",
    "    \n",
    "        # Sources\n",
    "        bc_set_source = set(bc_source[0])\n",
    "        id_set_source = set(id_source[0])\n",
    "        od_set_source = set(od_source[0])\n",
    "        \n",
    "        # Find common sources\n",
    "        common_sources = id_set_source & od_set_source # Returns indexes\n",
    "    \n",
    "        # Sinks\n",
    "        bc_set_sink = set(bc_sink[0])\n",
    "        id_set_sink = set(id_sink[0])\n",
    "        od_set_sink = set(od_sink[0])\n",
    "        \n",
    "        # Find common sinks\n",
    "        common_sinks = id_set_sink & od_set_sink # Returns indexes\n",
    "\n",
    "        # Identify not a heatwave\n",
    "        bc_neither = np.where(day['BC'] == 0.0)\n",
    "        cc_neither = np.where(day['CC'] == 0.0)\n",
    "        dc_neither = np.where(day['DC'] == 0.0)\n",
    "        id_neither = np.where(day['ID'] == 0.0) \n",
    "        od_neither = np.where(day['OD'] == 0.0)\n",
    "\n",
    "        # Neither\n",
    "        bc_set_neither = set(bc_neither[0])\n",
    "        cc_set_neither = set(cc_neither[0])\n",
    "        dc_set_neither = set(dc_neither[0])\n",
    "        id_set_neither = set(id_neither[0])\n",
    "        od_set_neither = set(od_neither[0])\n",
    "\n",
    "        # Find common zeros\n",
    "        common_zeros = bc_set_neither & cc_set_neither & dc_set_neither & id_set_neither & od_set_neither # Returns indexes\n",
    "\n",
    "        if len(common_sources) == len(day) | len(common_sinks) == len(day):\n",
    "            neither = list(index_set)\n",
    "            day.loc[neither, 'Type'] = 0\n",
    "            data.append(day)\n",
    "        else:\n",
    "            # Identify nodes part of the heatwave but not a source or sink\n",
    "            union = common_sources | common_sinks\n",
    "            part_of = index_set - union - common_zeros\n",
    "            \n",
    "            sources = list(common_sources)\n",
    "            sinks = list(common_sinks)\n",
    "            neither = list(common_zeros)\n",
    "            part = list(part_of)\n",
    "            day.loc[sources, 'Type'] = 1\n",
    "            day.loc[part, 'Type'] = 2\n",
    "            day.loc[sinks, 'Type'] = 3\n",
    "            day.loc[neither, 'Type'] = 0\n",
    "    \n",
    "            data.append(day)\n",
    "\n",
    "    df = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d0b681-be04-4e42-8b37-466fcb69d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(year):\n",
    "    df1 = pd.read_csv('data/' + year + '/md_zero_' + year + '_norm.csv')\n",
    "    df2 = pd.read_csv('data/' + year + '/md_!zero_' + year + '_norm.csv')\n",
    "\n",
    "    x = sources_sinks(df1)\n",
    "    y = sources_sinks(df2)\n",
    "\n",
    "    x.to_csv('data/' + year + '/clean_labeled_zero_'+ year + '.csv', index=False)\n",
    "    y.to_csv('data/' + year + '/clean_labeled_!zero_'+ year + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c6124-f7a8-4072-86a7-a36b8fe3e1d9",
   "metadata": {},
   "source": [
    "## Compiler/ executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef24a8d-e809-4e5a-ae92-a42ebf50a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d7250e-7746-47b1-ad38-e88bfc2fc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(year):\n",
    "    year = str(year)\n",
    "    folder_path = \"data/\" + year  # Replace with the desired path\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    is_heatwave_df(year)\n",
    "    comp_net_df(year)\n",
    "    cleaning(year)\n",
    "    normalize(year)\n",
    "    labeling(year) # Only included when labels are manually assigned (takes too long to do for each year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69b690-4784-47a5-987c-5e7da2909160",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
